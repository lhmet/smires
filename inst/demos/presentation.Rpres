Calculating Flow and Intermittency Metrics based on streamflow data
========================================================
author:
date:
font-family: 'Helvetica'
width: 1600
height: 1000

An attempt to generalize the calculation of flow and drought statistics using R.

CEH Wallingford, 2017-02-13

<br><br><br><br>

 - Introducing the package `smires`
 - Determining intermittency
 - From discharge time series to flow metrics (workflow & example)
 - Verifying MAM7 against lfstat
 - Outlook






The package smires
========================================================

Install the current development version (``r packageVersion("smires")``) of smires from [github](https://github.com/mundl/smires).

```{r, eval=FALSE, include=TRUE}
library(devtools)
install_github("mundl/smires")
```

Load the package, which already has two UK data sets included: Balder at [Balderhead Reservoir](http://nrfa.ceh.ac.uk/data/station/info/25022) and Ampney Brook at [Ampney St Peter](http://nrfa.ceh.ac.uk/data/station/info/39099).

```{r}
library(smires)
```

The time series are objects of class `tibble` which are similar to `data.frames`. The first column is the index of the time series, it is named `time` and is of class `Date`. The second column names `discharge` contains the observed streamflow discharges. Throughout the analysis, further variables (columns) can be appended.

****

```{r}
balder
```



Determining Intermittency
========================================================
left: 55%

```{r, echo=FALSE}
library(ggplot2)
x <- ampneyBrook
is.im <- is.intermittent
```


1 l/s, 5 days, 1 year (not necessarily consecutive)
```{r, message=TRUE}
is.im(x, threshold = 0.001)
is.im(x, ndays = 386, con = F)
is.im(x, ndays = 387, con = F)

is.im(x, ndays = 144, con = T)
is.im(x, ndays = 143, con = T)
```

***

```{r, echo=FALSE, fig.height = 6, fig.width = 5, out.width="980px", out.height="978px", dpi=300}
plot_events(find_events(ampneyBrook, threshold = 0.001), label = FALSE) +
  theme(legend.position = "none")
```


Calculating Flow Metrics: the general Approach
========================================================
<!--  Hypothesis: Almost every flow and drought statistic can be calculated in 4 steps.
Starting with streamflow discharge, new variables can be derived in step 2 and 3. -->
<br><br>

1. **preproces** time series of discharge (moving average, interpolate `NA`s, ...)

2. **identify** independent **events**
  - detect events based on a threshold flow (any value in `[0, Inf]`)
  - derive new variables for each event (e.g. duration, volume, start)

3. calculate **summary statistics** for arbitrary **periods** (e.g.  months, seasons, years or whole ts)
  - aggregate variables (e.g. discharge, duration, ...) per period using aggregation functions (max, min, median, ...)

4. calculate **final metric**
 - aggregate all periods to a single value (quantile, median, ...)
 - or show its distribution


1. Preprocessing
========================================================

We are quite strict about missing streamflow observations (`NA`s). When calculating a metric, periods containing `NA` values are dropped and a warning is risen. Short gaps (of length `approx.missing`) can be interpolated.

To check if there are implausibly low observations (negative discharges, values below accuracy) the user can supply an optional argument `accuracy`.

```{r, include=FALSE}
balder$discharge[c(1, 100:102, 200)] <- NA
```


```{r, message=TRUE}
discharge <- check_ts(balder, accuracy = 0.005)
```


2. Identifying events
========================================================

```{r, message=TRUE}
events <- find_events(discharge, threshold = 0.001)
events
```

2. Identifying events
========================================================

Each period (in our example: years) can contain zero or more events. There can be periods with zero duration, e.g. if a year is wet 365 days, the dry period has a duration of zero days.

```{r,  fig.height = 3, fig.width = 10, out.width="1960px", out.height="490px", dpi=300}
plot_events(events)
```



3. Aggregating per period
========================================================

Each variable (e.g. duration, volume, onset) calculated before has a distribution, there can be more than one event per period.
```{r, fig.height = 3, fig.width = 10, out.width="1960px", out.height="490px", dpi=300,, echo=FALSE}
ggplot(filter(split_events(assign_period(events)), !is.na(state)), aes(duration)) +
  geom_density(trim = TRUE) +
  facet_wrap(~ state) +
  scale_x_continuous() + geom_rug(aes(col = year)) +
  labs(x = "Duration of an event (days)",
       title = "How are durations distributed, regardless of the period?")
```

3. Aggregating per period
========================================================

We can choose:
 - which variables `vars()` to aggregate and
 - which aggregation functions `funs()` to use.


```{r}
metricP <- events %>% assign_period() %>%
  split_events() %>%
  drop_na_periods(year) %>%  group_by(year, state) %>%
  summarise_at(vars(duration), funs(max))

metricP
```


3. Aggregating per period
========================================================

```{r, fig.height = 3, fig.width = 5, out.width="980px", out.height="490px", dpi=300}
plot_period(metricP, type = "ts")
```

By aggregating (taking the maximum) all values of a single period we obtain one value per period, the maximum duration of an event per period.

***

```{r, fig.height = 3, fig.width = 5, out.width="980px", out.height="490px", dpi=300}
plot_period(metricP, type = "dist")
```



4. Calculate the final metric
========================================================
Same procedure as before, choose:
 - which variables `vars()` to aggregate and
 - which aggregation functions `funs()` to use.


```{r}
metric <- metricP %>% group_by(state) %>%
  summarise_at(vars(-year, -state), funs(mean))
metric
```

E.g. the mean annual maximum duration (correct?) of dry states at Balderhead Reservoir is ``r round(filter(metric, state == "no-flow")$duration, 1)`` days. But we only considered 4.3 years of records...

Dimensions
========================================================
Thinking about the dimensions revealed, that I designed the `find_event()` function poorly. Assigning the periods should happen before step 3.

|    | Operation                   | What Varies  | Dimension |  after Operation  | Pseudo-code   |
|:---|:--|:---|:--|:----|:--|
| 1. | preprocessing   | functions   | 2 | time, discharge   | ` x <- check_ts(...)` |
| 2. | assign periods   | period   | 3 | time, period, discharge | `xx <- per(e, period)` |
| 3. | event detection   | threshold, *n* functions (defining *n* new vars)   | *n* + 2 | event, period,  *n* vars   | `e <- find_events(xx, threshold)` |
| 4. | metrics per periods | *m* aggregation functions  | *n* * *m* + 1 | period, *n* * *m* vars  | `p <- summarise(e, period, funs)` |
| 5. | metric per station  | *o* aggregation functions   |*n* * *m* * *o* | *n* * *m* * *o* vars | `summarise(p, funs)` |



In steps 4 and 5 basically the same operation is carried out to reduce a dimension: the same function can be used.

Step 3 is optional (*n* = 1), e.g. for low flow statistics like *MAM7* no events are required.

Most of the time *n*, *m* and *o* = 1 resulting in a **single final metric**.




Example: Verifying MAM7 against lfstat
========================================================
```{r, include=FALSE}
library(lfstat)
mam7 <- function(x)
{
  apply.seasonal(x, varying = "yearly", aggregate = mean)
}

rm(ampneyBrook)
discharge <- ampneyBrook
```



### lfstat
```{r}
x <- with(ampneyBrook, xts(ma(discharge, n = 7, sides = 1), order.by = time))
(lfstat <- mam7(x["1984::"]))
```


### smires
```{r, message = TRUE, eval=FALSE}
discharge$discharge <- ma(discharge$discharge, n = 7, sides = 1)
p <- assign_period(discharge) %>% drop_na_periods(year)
metricP <- p %>% summarise_at(vars(discharge), funs(min))
smires <- metricP %>% summarise_at(vars(discharge), funs(mean))
```

### comparison
```{r, message = TRUE, eval = FALSE}
identical(lfstat, smires$discharge[1])
```



4. Outlook
========================================================
<br><br>
 1.  **Define requirements** for every metric: length of record, max number of missing values
 2. Start **implementing all suggested metrics**.
 3. Include **data submitted** by countries. Can we include it in the package? If not, distribute as rda-object which can easily be loaded.
 4. **Report** every suggested metric for every submitted time series.
 5. Wrapping the whole workflow into a single **wrapper function**. Difficult because current code heavily depends on non-standard evaluation.
 6. Work on **naming** and wording (dry/wet?). Help! : )
 7. **Unit tests**: test functions against trusted results. Can we outsource this task?
 8. As soon as the implementation is almost complete, induce **object orientation** and export methods. Also start working on the documentation.
